model:
  prompt_encoder:
    name: dummy
#    checkpoint: checkpoints/sam_vit_h_4b8939.pth
#    use_sam_checkpoint: True
  hidden_size: 256
  pe_in_channels: 256
  clip_in_channels: 1024
dataset:
  train:
    name: coco
    instances_path: data/annotations/instances_train2017.json
    #emb_dir: /ext/stalla/LabelAnything/embeddings
    clip_emb_dir: &clip_emb_dir dummy
    img_dir: &img_dir /ext/stalla/LabelAnything/images/train2017
    num_examples: &num_examples 3
    do_subsample: False
    add_box_noise: True
    max_points_annotations: 50
    max_points_per_annotation: 10
    load_gts: False
  val:
    name: coco
    instances_path: data/annotations/instances_val2017.json
    clip_emb_dir: *clip_emb_dir
    num_examples: *num_examples
    img_dir: *img_dir
    do_subsample: False
    add_box_noise: False
    max_points_annotations: 50
    max_points_per_annotation: 10
    load_gts: False
dataloader:
  batch_size: 2
  drop_last: False
  shuffle: False
criterion:
  t: .7
  norm: True
optimizer:
  lr: 1e-4
scheduler:
  patience: 2
  factor: 0.1
early_stopping:
  out_dir: /leonardo_work/IscrC_PENELOPE/raffaele/LabelAnything/data/experiment/pretrain_pe
  patience: 5
train_loop:
  num_epochs: 50