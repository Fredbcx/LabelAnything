{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emanuele/LabelAnything/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(str(Path.cwd().parent / \"label_anything\"))\n",
    "sys.path.append(str(Path.cwd().parent / \"label_anything\" / \"data\"))\n",
    "\n",
    "import os\n",
    "from label_anything.data.test import LabelAnythingTestDataset\n",
    "from label_anything.data.utils import BatchKeys\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "from pycocotools import mask as mask_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainMriTestDataset(LabelAnythingTestDataset):\n",
    "    num_classes = 2\n",
    "\n",
    "    def __init__(self, annotations, img_dir, transform=None):\n",
    "        super().__init__()\n",
    "        with open(annotations, \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir  # data/raw/lgg-mri-segmentation/kaggle_3m/\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations[\"images\"])\n",
    "\n",
    "    def _get_image(self, image_info):\n",
    "        image_path = os.path.join(self.img_dir, image_info[\"url\"])\n",
    "        img = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  # 3 x h x w\n",
    "        return img, (img.shape[1], img.shape[2])\n",
    "\n",
    "    def _get_gt(self, annotation_info):\n",
    "        mask = mask_utils.decode(annotation_info[\"segmentation\"])  #\n",
    "        bbox = (annotation_info[\"bbox\"],)  # [x, y, w, h]\n",
    "        if self.transform:\n",
    "            mask = self.transform(mask)\n",
    "            bbox = torch.tensor(bbox)\n",
    "        return {\"mask\": mask, \"bbox\": bbox}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_info = self.annotations[\"images\"][idx]\n",
    "        annotation_info = self.annotations[\"annotations\"][idx]\n",
    "        image, size = self._get_image(image_info)\n",
    "        gt = self._get_gt(annotation_info)\n",
    "        return {\n",
    "            BatchKeys.IMAGES: image,\n",
    "            BatchKeys.DIMS: size,\n",
    "        }, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "def extract_prompts(self):\n",
    "    images = [\n",
    "        self._get_image(self.train_channels_folder, filename)\n",
    "        for filename in self.prompt_images\n",
    "    ]\n",
    "    sizes = torch.stack([torch.tensor(x.shape[1:]) for x in images])\n",
    "    images = [self._transform(image) for image in images]\n",
    "    images = torch.stack(images)\n",
    "    masks = [\n",
    "        self._get_gt(self.train_gt_folder, filename) for filename in self.prompt_images\n",
    "    ]\n",
    "    masks = torch.stack(masks)\n",
    "    masks = one_hot(masks.long(), 3).permute(0, 3, 1, 2).float()\n",
    "    contains_crop = (masks == 1).sum(dim=(1, 2)) > 0\n",
    "    contains_weed = (masks == 2).sum(dim=(1, 2)) > 0\n",
    "    flag_masks = torch.stack([contains_crop, contains_weed]).T\n",
    "\n",
    "    prompt_dict = {\n",
    "        BatchKeys.IMAGES: images,\n",
    "        BatchKeys.PROMPT_MASKS: masks,\n",
    "        BatchKeys.FLAG_MASKS: flag_masks,\n",
    "        BatchKeys.DIMS: sizes,\n",
    "    }\n",
    "    return prompt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = \"/home/emanuele/LabelAnything/data/annotations/brain_mri.json\"\n",
    "img_dir = \"/home/emanuele/LabelAnything/data/raw/lgg-mri-segmentation/kaggle_3m/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = BrainMriTestDataset(annotations, img_dir, transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images: torch.Size([4, 3, 256, 256])', 'dims: [tensor([256, 256, 256, 256]), tensor([256, 256, 256, 256])]']\n",
      "['mask: torch.Size([4, 1, 256, 256])', 'bbox: torch.Size([4, 1, 4])']\n"
     ]
    }
   ],
   "source": [
    "data_dict, gt = next(iter(dataloader))\n",
    "\n",
    "print(\n",
    "    [\n",
    "        f\"{k}: {v.size() if isinstance(v, torch.Tensor) else v}\"\n",
    "        for k, v in data_dict.items()\n",
    "    ]\n",
    ")\n",
    "print([f\"{k}: {v.size() if isinstance(v, torch.Tensor) else v}\" for k, v in gt.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(os.path.join(img_dir, annotations_file[\"images\"][0][\"url\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = annotations_file[\"annotations\"][0][\"segmentation\"]\n",
    "bbox = annotations_file[\"annotations\"][0][\"bbox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotations, \"r\") as f:\n",
    "    annotations_file = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "i = mask_utils.decode(seg)\n",
    "# convert bbox to tensor\n",
    "bbox = torch.tensor(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_category(annotations):\n",
    "    image_to_category = {}\n",
    "    for annotation in annotations[\"annotations\"]:\n",
    "        image_id = annotation[\"image_id\"]\n",
    "        category_id = annotation[\"category_id\"]\n",
    "        image_to_category[image_id] = category_id\n",
    "    return image_to_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = image_to_category(annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
