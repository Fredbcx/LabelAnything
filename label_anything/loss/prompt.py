import torch
import torch.nn as nn

from torch.nn.functional import normalize, binary_cross_entropy
from einops import rearrange


class PromptContrastiveLoss(nn.Module):
    def __init__(self):
        """
        Computes the contrastive loss of the class prompts generated for each example in the support set. 
        """
        super().__init__()
        
    def forward(self, class_embeddings: torch.Tensor, flag_masks, flag_points, flag_boxes):
        """
        Arguments:
            class_embeddings (torch.Tensor): the class embeddings generated by the model (B x M x C x D)
        """
        ignore_index = -100
        B, M, C, D = class_embeddings.shape
        
        flag_points = flag_points.sum(dim=3) # B x M x C x N -> B x M x C
        flag_boxes = flag_boxes.sum(dim=3) # B x M x C x N -> B x M x C
        flags = (flag_masks + flag_points + flag_boxes)# B x M x C
        flags[:, :, 0] = 1 # The first class is always the background and no prompts are generated for it
        flags = rearrange(flags, "b m c -> b (m c) 1")
        flags = (~(flags.repeat(1, 1, M*C).bool())).float()
        flags = (~(flags + rearrange(flags, " b c d -> b d c")).bool())
        flags = torch.triu(flags)
        
        class_embeddings = rearrange(class_embeddings, "b m c d -> b (m c) d")
        dot_products = class_embeddings @ rearrange(class_embeddings, " b c d -> b d c")
        dot_products = normalize(dot_products, dim=1)
        
        contrastive_matrix = torch.eye(C, device=class_embeddings.device)
        contrastive_matrix = contrastive_matrix.unsqueeze(0).repeat(B, M, M)
        cross_entropy_loss = binary_cross_entropy(dot_products, contrastive_matrix, reduction="none")
        return cross_entropy_loss[flags].mean()
        
        
        
        
        
