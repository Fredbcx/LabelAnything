import torch
import torch.nn as nn

from torch.nn.functional import normalize, binary_cross_entropy
from einops import rearrange


class PromptContrastiveLoss(nn.Module):
    def __init__(self):
        """
        Computes the contrastive loss of the class prompts generated for each example in the support set. 
        """
        
    def forward(self, class_embeddings: torch.Tensor, flag_masks, flag_points, flag_boxes):
        """
        Arguments:
            class_embeddings (torch.Tensor): the class embeddings generated by the model (B x M x C x D)
        """
        ignore_index = -100
        B, M, C = class_embeddings.shape
        
        flag_points = flag_points.sum(dim=3) # B x M x C x N -> B x M x C
        flag_boxes = flag_boxes.sum(dim=3) # B x M x C x N -> B x M x C
        flags = (flag_masks + flag_points + flag_boxes)# B x M x C
        flags = rearrange(flags, "b m c -> b (m c)")
        flags = flags.repeat(1, M*C)
        flags = ~(flags + flags.T).bool() 
        
        class_embeddings = rearrange(class_embeddings, "b m c d -> b (m c) d")
        dot_products = class_embeddings @ class_embeddings.T
        dot_products = normalize(dot_products, dim=1)
        
        contrastive_matrix = torch.eye(C, device=class_embeddings.device)
        contrastive_matrix = contrastive_matrix.unsqueeze(0).repeat(B, M, M)
        contrastive_matrix = contrastive_matrix.masked_fill(flags, ignore_index)
        cross_entropy_loss = binary_cross_entropy(dot_products, contrastive_matrix, ignore_index=ignore_index, reduction="none")
        return cross_entropy_loss.mean(dim=(1, 2)).mean()
        
        
        
        
        
